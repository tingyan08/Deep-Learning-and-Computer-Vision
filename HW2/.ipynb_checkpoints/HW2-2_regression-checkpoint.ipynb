{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSoklTpd64He"
   },
   "source": [
    "## Homework 02-2: due 2022/03/31 23:59 (70%)\n",
    "\n",
    "### - Tutorial : https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
    "\n",
    "### - After you go through the tutorials, you should be able to work on this assignment.\n",
    "\n",
    "### - Please answer the following questions and work directly on this jupyter notebook.\n",
    "\n",
    "### - Make sure the code can be run and show the result and figures properly.\n",
    "\n",
    "### - Please write down your observation with markdown in this notebook briefly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz9X8m2NJwdj"
   },
   "source": [
    "#### You will train a regression model in this part. The data is **Concrete Compressive Strength Dataset**, you can find the details of each column at https://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength. In this part, please predict the **strength of concrete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gFMwtIeAJwdj"
   },
   "outputs": [],
   "source": [
    "# Import necessory modules\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "BpSVExq8Jwdl"
   },
   "outputs": [],
   "source": [
    "# For reproduce the result\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFN95MvCJwdl"
   },
   "source": [
    "### 1. Define the model and dataset (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1xJhvMbJwdm"
   },
   "source": [
    "#### 1.1 Please follow the tutorial to create a class ***ConcreteDataset***, for loading the data you need and also do the ***Min-Max scaling*** to the ***feature and label***. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "f6qRprzVJwdm"
   },
   "outputs": [],
   "source": [
    "class ConcreteDataset():\n",
    "    def __init__():\n",
    "        \n",
    "        \n",
    "    def __len__():\n",
    "    \n",
    "    def __getitem__():\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVC7FOQkJwdn"
   },
   "source": [
    "#### 1.2 By following the tutorial, try to create a class ***MLP(Neural Network)*** with ***three hidden layers*** as your network architecture. Also, for the convenience of implementation, please set the numbers of hidden nodes and the activation functions as input variables to the foward functions.(5%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "vWPTZrVdJwdo"
   },
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__():\n",
    "        \n",
    "    def forward():\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPm02AdTJwdp"
   },
   "source": [
    "### 2. Train the model (60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxwWJdZkJwdq"
   },
   "source": [
    "#### 2.1 Please load the ***train.csv/ validation.csv*** in ./data, and turn them into dataloader with batch size 64 and determine whether shuffle or not. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK_23b6YJwdu",
    "tags": []
   },
   "source": [
    "#### 2.2 Create two MLP model from the table below and ***print the model*** (10%):\n",
    "| | Hidden Layer 1| Hidden Layer 2| Hidden Layer 3| Activation in each hidden nodes | Activation for output |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|***Model1***| 256 | 64 | 8 | Tanh | Tanh |\n",
    "|***Model1***| 64 | 16 | 8 | Sigmoid | Identity |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_BjKmc1Jwdv"
   },
   "source": [
    "#### 2.3 Train the above two models with the same hyperpameters below and do the validation in every epoch. Choose the appropriate type of loss function according to the task. (25%)\n",
    "##### (Note. You should record the training/validation loss every epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uibgj0BQJwdv"
   },
   "source": [
    "\n",
    "| | Learning rate | epochs | optimizer | momentum |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|***Hyperparameter***| 0.01 | 300 | SGD | 0.9 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C9_OteKJwdv"
   },
   "source": [
    "#### 2.4 Please include the plot that shows how the training/validation loss vary with the training epoch. Show the plots using the above two models and write down the observation. (5%)\n",
    "##### Here is the example figure\n",
    "![train_ex.png](https://github.com/tingyan08/Deep-Learning-and-Computer-Vision/blob/main/HW2/img/HW2-2.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 From the observation of the previous question, please determine an appropriate epoch and retrain the two models to avoid overfitting. Also, draw the loss plot of two models and save the last model as ***model1.pth*** and ***model2.pth***. (10%)\n",
    "##### (Note. You should reload the models and the optimizer, otherwise, you will resume from the previous stop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj7T-StIJwdw",
    "tags": []
   },
   "source": [
    "#### 2.6 Please load the checkpoints saved from previous question and calculate the mean squared error on test dataset for two models respectively. Also, make a dataframe with target and prediction like below and save it as ***regression.csv*** (5%)\n",
    "|Target|Model1|Model2|\n",
    "|:---:|:---:|:---:|\n",
    "|24.05|26.35|27.04|\n",
    "|21.67|32.78|21.95|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW2-2_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
