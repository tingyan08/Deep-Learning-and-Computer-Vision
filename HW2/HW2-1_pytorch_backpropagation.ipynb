{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 02-1: due 2022/03/31 14:10 (30%)\n",
    "\n",
    "#### - In this part, you should calculate the forward pass and backpropagation manually and there is no need for any coding.\n",
    "\n",
    "#### - Please scan your hand-writting calculation and save it as HW2-1.pdf\n",
    "\n",
    "#### - By running the following script, you can check your answer and observe how to do the backpropagation in PyTorch.\n",
    "\n",
    "#### - You can change the iterations in script to observe how will the loss change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Please do the forward pass and backpropagation with a neural network as below, the input is 2 and the target is 1. Also, calculate the loss with ***half and the sum square error***. Please update the parameters twice and use the learning rate 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_ex.png](https://github.com/tingyan08/Deep-Learning-and-Computer-Vision/blob/main/HW2/img/HW2-1.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([2], dtype= torch.float32)\n",
    "y = torch.tensor([1], dtype= torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half of the sum square error\n",
    "def loss(y, pred):\n",
    "    return ((pred-y)**2).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameters\n",
    "def show_parameters(i, X, model):\n",
    "    print(f\"Iters {i}\")\n",
    "    print(\"Input:\")\n",
    "    print(X)\n",
    "    for layer_name, layers in model.named_modules():\n",
    "        print(\"-----------------------\")\n",
    "        if not isinstance(layers, nn.Sequential):\n",
    "            for param_name, param in layers.named_parameters():\n",
    "                print(f\"{layer_name} {param_name} {param}\")\n",
    "                print(f\"{layer_name} {param_name} Gradient\")\n",
    "                print(param.grad)\n",
    "            print(f\"{layer_name} output:\")\n",
    "            X = layers(X)\n",
    "            print(X)\n",
    "            \n",
    "    print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for name, i in model.named_modules():\n",
    "        if isinstance(i, nn.Linear):\n",
    "            nn.init.constant_(i.weight.data, 1)\n",
    "            nn.init.constant_(i.bias.data, -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters 0\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[1.],\n",
      "        [1.]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[14.],\n",
      "        [14.]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.5000, -0.5000], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([7., 7.])\n",
      "Layer1 output:\n",
      "tensor([1.5000, 1.5000], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([1.5000, 1.5000], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[5.2500, 5.2500],\n",
      "        [5.2500, 5.2500]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5000, -0.5000], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([3.5000, 3.5000])\n",
      "Layer2 output:\n",
      "tensor([2.5000, 2.5000], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([2.5000, 2.5000], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[1., 1.]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[8.7500, 8.7500]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5000], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([3.5000])\n",
      "Layer3 output:\n",
      "tensor([4.5000], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 1\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.8600],\n",
      "        [0.8600]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[5.0691],\n",
      "        [5.0691]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.5700, -0.5700], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([2.5346, 2.5346])\n",
      "Layer1 output:\n",
      "tensor([1.1500, 1.1500], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([1.1500, 1.1500], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9475, 0.9475],\n",
      "        [0.9475, 0.9475]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[1.5381, 1.5381],\n",
      "        [1.5381, 1.5381]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5350, -0.5350], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([1.3375, 1.3375])\n",
      "Layer2 output:\n",
      "tensor([1.6443, 1.6443], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([1.6443, 1.6443], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.9125, 0.9125]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[2.4101, 2.4101]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5350], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([1.4658])\n",
      "Layer3 output:\n",
      "tensor([2.4658], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 2\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.8093],\n",
      "        [0.8093]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[2.8667],\n",
      "        [2.8667]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.5953, -0.5953], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([1.4333, 1.4333])\n",
      "Layer1 output:\n",
      "tensor([1.0233, 1.0233], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([1.0233, 1.0233], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9321, 0.9321],\n",
      "        [0.9321, 0.9321]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.7868, 0.7868],\n",
      "        [0.7868, 0.7868]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5484, -0.5484], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.7689, 0.7689])\n",
      "Layer2 output:\n",
      "tensor([1.3592, 1.3592], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([1.3592, 1.3592], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8884, 0.8884]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[1.1764, 1.1764]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5497], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.8654])\n",
      "Layer3 output:\n",
      "tensor([1.8654], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 3\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.7806],\n",
      "        [0.7806]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[1.7852],\n",
      "        [1.7852]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.6097, -0.6097], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([0.8926, 0.8926])\n",
      "Layer1 output:\n",
      "tensor([0.9516, 0.9516], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([0.9516, 0.9516], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9243, 0.9243],\n",
      "        [0.9243, 0.9243]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.4595, 0.4595],\n",
      "        [0.4595, 0.4595]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5561, -0.5561], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.4829, 0.4829])\n",
      "Layer2 output:\n",
      "tensor([1.2030, 1.2030], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([1.2030, 1.2030], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8766, 0.8766]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[0.6626, 0.6626]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5583], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.5508])\n",
      "Layer3 output:\n",
      "tensor([1.5508], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 4\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.7628],\n",
      "        [0.7628]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[1.1615],\n",
      "        [1.1615]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.6186, -0.6186], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([0.5808, 0.5808])\n",
      "Layer1 output:\n",
      "tensor([0.9070, 0.9070], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([0.9070, 0.9070], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9197, 0.9197],\n",
      "        [0.9197, 0.9197]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.2864, 0.2864],\n",
      "        [0.2864, 0.2864]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5609, -0.5609], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.3158, 0.3158])\n",
      "Layer2 output:\n",
      "tensor([1.1073, 1.1073], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([1.1073, 1.1073], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8700, 0.8700]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[0.4019, 0.4019]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5638], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.3629])\n",
      "Layer3 output:\n",
      "tensor([1.3629], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 5\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.7512],\n",
      "        [0.7512]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[0.7740],\n",
      "        [0.7740]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.6244, -0.6244], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([0.3870, 0.3870])\n",
      "Layer1 output:\n",
      "tensor([0.8779, 0.8779], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([0.8779, 0.8779], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9168, 0.9168],\n",
      "        [0.9168, 0.9168]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.1853, 0.1853],\n",
      "        [0.1853, 0.1853]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5641, -0.5641], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.2111, 0.2111])\n",
      "Layer2 output:\n",
      "tensor([1.0457, 1.0457], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([1.0457, 1.0457], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8660, 0.8660]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[0.2549, 0.2549]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5674], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.2437])\n",
      "Layer3 output:\n",
      "tensor([1.2437], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 6\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.7434],\n",
      "        [0.7434]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[0.5231],\n",
      "        [0.5231]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.6283, -0.6283], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([0.2616, 0.2616])\n",
      "Layer1 output:\n",
      "tensor([0.8586, 0.8586], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([0.8586, 0.8586], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9149, 0.9149],\n",
      "        [0.9149, 0.9149]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.1227, 0.1227],\n",
      "        [0.1227, 0.1227]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5662, -0.5662], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.1429, 0.1429])\n",
      "Layer2 output:\n",
      "tensor([1.0049, 1.0049], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([1.0049, 1.0049], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8634, 0.8634]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[0.1664, 0.1664]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5699], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.1655])\n",
      "Layer3 output:\n",
      "tensor([1.1655], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 7\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.7382],\n",
      "        [0.7382]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[0.3567],\n",
      "        [0.3567]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.6309, -0.6309], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([0.1784, 0.1784])\n",
      "Layer1 output:\n",
      "tensor([0.8455, 0.8455], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([0.8455, 0.8455], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9137, 0.9137],\n",
      "        [0.9137, 0.9137]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.0825, 0.0825],\n",
      "        [0.0825, 0.0825]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5676, -0.5676], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.0976, 0.0976])\n",
      "Layer2 output:\n",
      "tensor([0.9775, 0.9775], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([0.9775, 0.9775], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8618, 0.8618]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[0.1107, 0.1107]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5715], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.1133])\n",
      "Layer3 output:\n",
      "tensor([1.1133], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 8\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.7346],\n",
      "        [0.7346]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[0.2447],\n",
      "        [0.2447]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.6327, -0.6327], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([0.1223, 0.1223])\n",
      "Layer1 output:\n",
      "tensor([0.8366, 0.8366], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([0.8366, 0.8366], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9129, 0.9129],\n",
      "        [0.9129, 0.9129]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.0561, 0.0561],\n",
      "        [0.0561, 0.0561]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5686, -0.5686], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.0670, 0.0670])\n",
      "Layer2 output:\n",
      "tensor([0.9589, 0.9589], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([0.9589, 0.9589], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8607, 0.8607]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[0.0746, 0.0746]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5727], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.0778])\n",
      "Layer3 output:\n",
      "tensor([1.0778], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 9\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.7322],\n",
      "        [0.7322]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[0.1685],\n",
      "        [0.1685]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.6339, -0.6339], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([0.0842, 0.0842])\n",
      "Layer1 output:\n",
      "tensor([0.8305, 0.8305], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([0.8305, 0.8305], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9123, 0.9123],\n",
      "        [0.9123, 0.9123]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.0383, 0.0383],\n",
      "        [0.0383, 0.0383]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5692, -0.5692], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.0462, 0.0462])\n",
      "Layer2 output:\n",
      "tensor([0.9461, 0.9461], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([0.9461, 0.9461], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8599, 0.8599]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[0.0508, 0.0508]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5735], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.0537])\n",
      "Layer3 output:\n",
      "tensor([1.0537], grad_fn=<AddBackward0>)\n",
      "===============================================\n",
      "Iters 10\n",
      "Input:\n",
      "tensor([2.])\n",
      "-----------------------\n",
      "-----------------------\n",
      "Layer1 weight Parameter containing:\n",
      "tensor([[0.7305],\n",
      "        [0.7305]], requires_grad=True)\n",
      "Layer1 weight Gradient\n",
      "tensor([[0.1163],\n",
      "        [0.1163]])\n",
      "Layer1 bias Parameter containing:\n",
      "tensor([-0.6347, -0.6347], requires_grad=True)\n",
      "Layer1 bias Gradient\n",
      "tensor([0.0581, 0.0581])\n",
      "Layer1 output:\n",
      "tensor([0.8263, 0.8263], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU1 output:\n",
      "tensor([0.8263, 0.8263], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer2 weight Parameter containing:\n",
      "tensor([[0.9119, 0.9119],\n",
      "        [0.9119, 0.9119]], requires_grad=True)\n",
      "Layer2 weight Gradient\n",
      "tensor([[0.0263, 0.0263],\n",
      "        [0.0263, 0.0263]])\n",
      "Layer2 bias Parameter containing:\n",
      "tensor([-0.5697, -0.5697], requires_grad=True)\n",
      "Layer2 bias Gradient\n",
      "tensor([0.0319, 0.0319])\n",
      "Layer2 output:\n",
      "tensor([0.9373, 0.9373], grad_fn=<AddBackward0>)\n",
      "-----------------------\n",
      "ReLU2 output:\n",
      "tensor([0.9373, 0.9373], grad_fn=<ReluBackward0>)\n",
      "-----------------------\n",
      "Layer3 weight Parameter containing:\n",
      "tensor([[0.8594, 0.8594]], requires_grad=True)\n",
      "Layer3 weight Gradient\n",
      "tensor([[0.0348, 0.0348]])\n",
      "Layer3 bias Parameter containing:\n",
      "tensor([-0.5740], requires_grad=True)\n",
      "Layer3 bias Gradient\n",
      "tensor([0.0371])\n",
      "Layer3 output:\n",
      "tensor([1.0371], grad_fn=<AddBackward0>)\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([(\"Layer1\", nn.Linear(1, 2)), \n",
    "                                   (\"ReLU1\", nn.ReLU()),\n",
    "                                   (\"Layer2\", nn.Linear(2, 2)), \n",
    "                                   (\"ReLU2\", nn.ReLU()),\n",
    "                                   (\"Layer3\", nn.Linear(2, 1))]))\n",
    "initialize_weights(model)\n",
    "lr = 0.01\n",
    "n_iters = 10\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "loss_list = []\n",
    "for i in range(n_iters+1):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X)\n",
    "    l = loss(pred, y)\n",
    "    loss_list.append(l.detach().numpy())\n",
    "    l.backward()\n",
    "    show_parameters(i, X, model)\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
